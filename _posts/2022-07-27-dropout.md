---
layout: post
title: Dropout
tags: [DL, CV, Color]
color: brown
author: Ahnho
excerpt_separator: <!--more-->
---


# Dropout

:: 딥러닝 학습의 문제중 하나인 Overfitting 해결하기 위한 방법중 하나이며, hidden layer의 일부 유닛을 동작하지 않게 하여 문제를 해결한다.


<!--more-->


## Overfitting Revisited

- 예제보다 feature가 더많으면 선형 모델은 overfitting 되는 경향이 있다.
- 위와 반대 되는경우 일반화가 잘됬다는 걸 기대할 수있지만 역으로 overfitting 될 수도있다.
- 일반화 가능성과 유연성 : bias-variance tradeoff

---



## Robustness through Perturbations

- 네트워크를 훈련시킬때 각각에 계층에 노이즈를 주입하면 입출력 매핑에서만 평활성이 강화된다.
- dropout: forward propagation 하는동안 각 내부층을 계산할때 노이즈 를 주입하는것을 포함 , 뉴런을 떨어 뜨리기때문에 drop out이라 불린다.
- 이때 노이즈를 주입하는법 :  각층의 기대값이 다른층을 고정하는 동안 노이즈가 없는 값과 같도록 편향되지 않는 방식으로 주입하는 것이다.
- 표준 드롭아웃 정규화에서는 드롭 아웃되지 않은 비률로 정규화를 진행시켜 각 계층을 약화 시킨다.



---



## Dropout in Practice

: ex) 5개의 hidden unit을 갖는 한개의 은닉층을 사용하는 MLP



- dropout의 확률 P로 hidden unit을 제거한다.(출력을 0으로 설정)_(아래 그림 참고)
- 제거된 hidden unit backpropgation시 사용되지않고 gradient에도 적용되지 않는다.
- 그럼 output layer 계산시 하나에 의존하는 경우는 없어진다. -> overfitting 문제해결
- dropout은 regularization 목적을 위해서 필요한 스킬이다.

![../_images/dropout2.svg](https://d2l.ai/_images/dropout2.svg)



- 일반적으로 테스트시 드롭아웃을 비활성화한다.
- 하지만 일부 연구에서 불확실성을 추정하기 위해 휴리스틱으로 테스트시 드롭아웃을 사용한다.

---

## Summary

- dropout은 차원수 조절, weight vector 크기 제어와 같이 overfitting문제를 해결하는 방법중 하나이다. 

---

## 참조

- https://d2l.ai/index.html
